import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split


from google.colab import drive
drive.mount('/content/drive')
dataset_path = '/content/drive/MyDrive/Colab Notebooks/HONLP/train-00000-of-00001.parquet'



nltk.download("stopwords")
nltk.download("punkt")
nltk.download('punkt_tab')
sns.set_style("darkgrid")
sns.set_context("notebook")
pd.set_option('display.precision', 2)





df = pd.read_parquet(dataset_path)


print(df.info())
df.head(5)


labels = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}


class_counts = df["label"].value_counts().sort_index()

plt.figure(figsize=(6, 4))
bars = plt.bar(class_counts.index.map(labels), class_counts.values, color=["blue", "orange", "green"])

for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(bar.get_height()),
             ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')

plt.xlabel("Emotion")
plt.ylabel("Number of Tweets")
plt.title("Number of Tweets per Emotion")
plt.xticks(rotation=0)
plt.show()





df["char_count"] = df["text"].apply(lambda x: len(x))

print(df["char_count"].describe())

plt.figure(figsize=(8, 5))
plt.hist(df["char_count"], bins=30, color="skyblue", edgecolor="black")
plt.xlabel("Char Count per Tweet")
plt.ylabel("Frequency")
plt.title("Distribution of Char Counts per Tweet")
plt.show()



char_counts_per_class = [df[df["label"] == label]["char_count"] for label in sorted(df["label"].unique())]

class_labels = [labels[label] for label in sorted(df["label"].unique())]

plt.figure(figsize=(8, 5))
plt.boxplot(char_counts_per_class, labels=class_labels, patch_artist=True, boxprops=dict(facecolor="lightblue"))
plt.xlabel("Emotion")
plt.ylabel("Character Count")
plt.title("Character Count Distribution per Emotion")
plt.show()



longest_tweets = df[df["char_count"]>400]
for _, row in longest_tweets.iterrows():
    print("\nTweet:\n")
    print(labels[row["label"]])
    print("\n")
    print(row["text"])





df = df[df["char_count"]<=400]


char_counts_per_class = [df[df["label"] == label]["char_count"] for label in sorted(df["label"].unique())]

class_labels = [labels[label] for label in sorted(df["label"].unique())]

plt.figure(figsize=(8, 5))
plt.boxplot(char_counts_per_class, labels=class_labels, patch_artist=True, boxprops=dict(facecolor="lightblue"))
plt.xlabel("Emotion")
plt.ylabel("Character Count")
plt.title("Character Count Distribution per Emotion")
plt.show()


df['tokens'] = df['text'].apply(word_tokenize)
df.head(5)


df['types'] = df.tokens.map(set)
df['types_count'] = df.types.map(len)
df.head(5)


types_counts_per_class = [df[df["label"] == label]["types_count"] for label in sorted(df["label"].unique())]

class_labels = [labels[label] for label in sorted(df["label"].unique())]

plt.figure(figsize=(8, 5))
plt.boxplot(types_counts_per_class, labels=class_labels, patch_artist=True, boxprops=dict(facecolor="lightblue"))
plt.xlabel("Emotion")
plt.ylabel("Vocabulary Length")
plt.title("Vocabulary Length Distribution per Emotion")
plt.show()





uniq_per_class = [df[df["label"] == label]["types_count"]/len(df[df["label"] == label]["types"]) for label in sorted(df["label"].unique())]

class_labels = [labels[label] for label in sorted(df["label"].unique())]

plt.figure(figsize=(8, 5))
plt.boxplot(uniq_per_class, labels=class_labels, patch_artist=True, boxprops=dict(facecolor="lightblue"))
plt.xlabel("Emotion")
plt.ylabel("Vocabulary Uniquness")
plt.title("Vocabulary Uniquness Distribution per Emotion")
plt.show()





all_tokens = [word for tokens in df["tokens"] for word in tokens]
token_freq = Counter(all_tokens)
token_df = pd.DataFrame(token_freq.items(), columns=["token", "frequency"])
token_df = token_df.sort_values(by="frequency", ascending=False).reset_index(drop=True)
token_df



import string

def count_punctuation(tokens):
    return sum(1 for token in tokens if token in string.punctuation)

punctuations = df["tokens"].apply(count_punctuation)

print(punctuations.unique())














vectorizer = TfidfVectorizer(ngram_range=(1,1), max_features = 1000)
xs_tf_idf = vectorizer.fit_transform(df.text).toarray()



X_train, X_test, y_train, y_test = train_test_split(xs_tf_idf, df['label'], test_size=0.2, random_state=42)

pca = PCA(n_components=2)
reduced_data_pca_train = pca.fit_transform(X_train)

reduced_data_pca_test = pca.transform(X_test)


plt.figure(figsize=(8, 5))
scatter = plt.scatter(reduced_data_pca_train[:, 0], reduced_data_pca_train[:, 1], c=y_train, cmap='viridis', alpha=0.6)

plt.title("PCA-Without removing stop words, with unigrams")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.legend(handles=scatter.legend_elements()[0], labels=[str(i) for i in sorted(y_train.unique())], title="Classes")

plt.show()






vectorizer_sw = TfidfVectorizer(ngram_range=(1,1), stop_words = "english", max_features = 1000)
xs_tf_idf_sw = vectorizer_sw.fit_transform(df.text).toarray()


X_train_sw, X_test_sw, y_train_sw, y_test_sw = train_test_split(xs_tf_idf_sw, df['label'], test_size=0.2, random_state=42)

pca = PCA(n_components=2)
reduced_data_pca_train_sw = pca.fit_transform(X_train_sw)

reduced_data_pca_test_sw = pca.transform(X_test_sw)


plt.figure(figsize=(8, 5))
scatter = plt.scatter(reduced_data_pca_train_sw[:, 0], reduced_data_pca_train_sw[:, 1], c=y_train_sw, cmap='viridis', alpha=0.6)

plt.title("PCA-With removing stop words, with unigrams")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.legend(handles=scatter.legend_elements()[0], labels=[str(i) for i in sorted(y_train_sw.unique())], title="Classes")

plt.show()





pca = PCA(n_components=100)
X_train_sw = pca.fit_transform(X_train_sw)
X_test_sw = pca.transform(X_test_sw)





X_train_sw, X_val_sw, y_train_sw, y_val_sw = train_test_split(X_train_sw, y_train_sw, test_size=0.2, random_state=42)





X_train_sw_reshaped = X_train_sw.reshape(X_train_sw.shape[0], X_train_sw.shape[1], 1)
X_test_sw_reshaped = X_test_sw.reshape(X_test_sw.shape[0], X_test_sw.shape[1], 1)
X_val_sw_reshaped = X_val_sw.reshape(X_val_sw.shape[0], X_val_sw.shape[1], 1)


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Embedding
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight


class_weight_dict = compute_class_weight('balanced', classes=np.unique(y_train_sw), y=y_train_sw)
class_weight_dict = dict(enumerate(class_weight_dict))


X_train_sw_reshaped.shape


model = Sequential([
    Conv1D(128, 3, activation='relu', input_shape=(X_train_sw_reshaped.shape[1], X_train_sw_reshaped.shape[2])),
    MaxPooling1D(pool_size=2),

    Conv1D(64, 3, activation='relu'),
    MaxPooling1D(pool_size=2),

    Conv1D(32, 3, activation='relu'),
    MaxPooling1D(pool_size=2),

    Flatten(),

    Dense(32, activation='relu'),
    Dropout(0.5),

    Dense(len(np.unique(y_train_sw)), activation='softmax')
])



model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)


history = model.fit(X_train_sw_reshaped, y_train_sw,
                    epochs=20,
                    batch_size=32,
                    validation_data=(X_val_sw_reshaped, y_val_sw),
                    class_weight=class_weight_dict,
                    callbacks=[early_stopping],
                    verbose=1
                    )





vectorizer_sw = TfidfVectorizer(ngram_range=(1,1), stop_words = "english", max_features = 5000)
xs_tf_idf_sw = vectorizer_sw.fit_transform(df.text).toarray()
X_train_sw, X_test_sw, y_train_sw, y_test_sw = train_test_split(xs_tf_idf_sw, df['label'], test_size=0.2, random_state=42)
X_train_sw, X_val_sw, y_train_sw, y_val_sw = train_test_split(X_train_sw, y_train_sw, test_size=0.2, random_state=42)


X_train_sw_reshaped = X_train_sw.reshape(X_train_sw.shape[0], X_train_sw.shape[1], 1)
X_test_sw_reshaped = X_test_sw.reshape(X_test_sw.shape[0], X_test_sw.shape[1], 1)
X_val_sw_reshaped = X_val_sw.reshape(X_val_sw.shape[0], X_val_sw.shape[1], 1)


class_weight_dict = compute_class_weight('balanced', classes=np.unique(y_train_sw), y=y_train_sw)
class_weight_dict = dict(enumerate(class_weight_dict))


model = Sequential([
    Conv1D(128, 3, activation='relu', input_shape=(X_train_sw_reshaped.shape[1], X_train_sw_reshaped.shape[2])),
    MaxPooling1D(pool_size=2),

    Conv1D(64, 3, activation='relu'),
    MaxPooling1D(pool_size=2),

    Conv1D(32, 3, activation='relu'),
    MaxPooling1D(pool_size=2),

    Conv1D(16, 3, activation='relu'),
    MaxPooling1D(pool_size=2),

    Flatten(),

    Dense(32, activation='relu'),
    Dropout(0.5),

    Dense(len(np.unique(y_train_sw)), activation='softmax')
])


model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)


history = model.fit(X_train_sw_reshaped, y_train_sw,
                    epochs=20,
                    batch_size=32,
                    validation_data=(X_val_sw_reshaped, y_val_sw),
                    class_weight=class_weight_dict,
                    callbacks=[early_stopping],
                    verbose=1
                    )


y_pred = model.predict(X_test_sw_reshaped)
y_pred_classes = np.argmax(y_pred, axis=1)


from sklearn.metrics import classification_report, confusion_matrix


print(classification_report(y_test_sw, y_pred_classes))


conf_matrix = confusion_matrix(y_test_sw, y_pred_classes)

labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()






misclassified_idx = np.where(y_test_sw != y_pred_classes)[0]

for i in range(5):
    idx = misclassified_idx[i]
    print(f"Text: {df.text.iloc[idx]}")
    print(f"True Label: {labels[y_test_sw.iloc[idx]]}")
    print(f"Predicted Label: {labels[y_pred_classes[idx]]}")
    print("-" * 80)
